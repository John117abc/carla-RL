"""
æ•°æ®é‡‡é›†ä¸ç¯å¢ƒæµ‹è¯•è„šæœ¬ã€‚
ç”¨äºéªŒè¯ CarlaEnv æ˜¯å¦èƒ½æ­£å¸¸ reset/stepï¼Œå¹¶æ‰“å°è§‚æµ‹ã€åŠ¨ä½œã€å¥–åŠ±ç­‰ä¿¡æ¯ã€‚
å¯é€‰ä¿å­˜å›¾åƒæˆ–çŠ¶æ€æ—¥å¿—ã€‚
"""

import os
import numpy as np
import cv2
from src.utils import (load_config,get_logger,
                       setup_code_environment)
from src.agents import OcpAgent
from src.buffer import Trajectory
# æ·»åŠ é¡¹ç›®æºç è·¯å¾„
# sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import gymnasium as gym
from src.envs.carla_env import CarlaEnv

logger = get_logger('train_ocp')


def save_image(obs, step: int, save_dir: str = "debug_images"):
    """ä¿å­˜è§‚æµ‹ä¸­çš„å›¾åƒï¼ˆå‡è®¾ obs æ˜¯ dict ä¸”åŒ…å« 'image'ï¼‰"""
    os.makedirs(save_dir, exist_ok=True)
    if isinstance(obs, dict) and 'image' in obs:
        img = obs['image']
        # å¦‚æœæ˜¯ (C, H, W)ï¼Œè½¬ä¸º (H, W, C)
        if img.shape[0] == 3:
            img = np.transpose(img, (1, 2, 0))
        img = (img * 255).astype(np.uint8)
        cv2.imwrite(f"{save_dir}/step_{step:04d}.png", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    elif isinstance(obs, np.ndarray) and obs.ndim == 3:
        img = (obs * 255).astype(np.uint8)
        if img.shape[0] == 3:
            img = np.transpose(img, (1, 2, 0))
        cv2.imwrite(f"{save_dir}/step_{step:04d}.png", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))


def main():
    logger.info('å¼€å§‹è¯»å–é…ç½®æ–‡ä»¶...')
    carla_config = load_config('configs/carla.yaml')['word_01']
    env_config = load_config('configs/env.yaml')
    sys_config = load_config('configs/sys.yaml')
    rl_config = load_config('configs/rl.yaml')
    train_config = rl_config['rl']
    device = setup_code_environment(sys_config)
    history = []
    logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– CARLA ç¯å¢ƒ...")
    env = CarlaEnv(
        render_mode=None,
        carla_config=carla_config,
        env_config=env_config
    )
    try:
        agent = OcpAgent(env=env, rl_config=rl_config, device=device)
        if train_config['continue_ocp']:
            logger.info("å¼€å§‹è¯»å–æ™ºèƒ½ä½“å‚æ•°...")
            checkpoint = agent.load(train_config["model_path_ocp"])
            if not env.is_eval:
                # è¯»å–å½’ä¸€åŒ–å‚æ•°
                env.ocp_normalizer.load_state_dict(checkpoint['ocp_normalizer'])

        logger.info("ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")
        logger.info(f"è§‚æµ‹ç©ºé—´: {env.observation_space}")
        logger.info(f"åŠ¨ä½œç©ºé—´: {env.action_space}")

        num_episodes = train_config["num_episodes"]
        global_step = 0
        episode = 0
        while episode < num_episodes:
            logger.info(f"\nå¼€å§‹ç¬¬ {episode + 1} è½®æµ‹è¯•...")
            state, info = env.reset()
            state = state['ocp_obs']
            logger.info(f"åˆå§‹è§‚æµ‹ç±»å‹: {type(state)}, å½¢çŠ¶/ç»“æ„: {get_obs_shape(state)}")
            total_reward = 0.0
            done = False
            states, actions, rewards, infos ,log_probs= [], [], [], [],[]
            initial_state = state.copy()
            while not done:
                action,log_prob = agent.select_action(state)
                next_obs, reward, _, _, info = env.step(action)
                next_state = next_obs['ocp_obs']
                done = info['collision'] or info['off_route'] or info['TimeLimit.truncated']
                total_reward += reward
                # æ•°æ®åŠ å…¥buffer
                actions.append(action)
                states.append(state[1])
                rewards.append(reward)
                log_probs.append(log_prob)
                infos.append(info)
                state = next_state

                # æ›´æ–°æƒ©ç½šå‚æ•°
                agent.update_penalty(env.step_count)
                # æ‰“å°å…³é”®ä¿¡æ¯
                if global_step % train_config["log_interval"] == 0:
                    logger.info(f"  Step {global_step}: reward={reward:.3f}, total={total_reward:.2f}")
                    if 'speed' in info:
                        logger.info(f"    é€Ÿåº¦: {info['speed']:.2f} km/h")

                if done:
                    logger.info(f"  Episode ç»“æŸ (info={info})")
                    break
                global_step += 1
            # è®¡ç®— total_cost å’Œ total_constraint
            total_cost, total_constraint = agent.compute_total_cost_and_constraint(states, actions)
            trajectory = Trajectory(initial_state=initial_state,
                                    states=states,actions=actions,
                                    rewards=rewards,
                                    infos=infos,
                                    total_cost=total_cost,
                                    total_constraint=total_constraint,
                                    path_id=env.current_path_id,
                                    horizon=len(states),
                                    log_probs = log_probs)
            # åŠ å…¥buffer
            agent.buffer.handle_new_trajectory(trajectory)

            # æ›´æ–°å‚æ•°
            loss = None
            if agent.buffer.should_start_training():
                loss = agent.update()
            logger.info(f"ç¬¬ {episode} è½®å®Œæˆï¼Œæ€»å¥–åŠ±: {total_reward:.2f}")

            if loss is not None:
                logger.info(f"è®­ç»ƒæŸå¤±: actor_loss:{loss['actor_loss']:.5f},critic_loss:{loss['critic_loss']:.5f},"
                            f"æƒ©ç½šå‚æ•°ï¼š{agent.init_penalty:.5f}")
                loss.update({
                    'global_step': global_step
                })
                history.append(loss)

            episode += 1

            # ä¿å­˜æ¨¡å‹
            if episode % train_config["save_freq"] == 0:
                logger.info(f"å¼€å§‹ä¿å­˜æ¨¡å‹ï¼š  Step {global_step}: reward={reward['total_reward']:.3f}, total={total_reward:.2f}")
                save_info = {
                    'rl_config':rl_config,
                    'global_step':global_step,
                    'map':env_config['world']['map'],
                    'history_loss':history,
                    'ocp_normalizer': env.ocp_normalizer.state_dict()
                }
                agent.save(save_info)

    except Exception as e:
        logger.error(f"ç¯å¢ƒè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        logger.info("\næ­£åœ¨å…³é—­ç¯å¢ƒ...")
        env.close()
        logger.info("æµ‹è¯•ç»“æŸã€‚")


def get_obs_shape(obs):
    """è¾…åŠ©å‡½æ•°ï¼šé€’å½’æ‰“å°è§‚æµ‹ç»“æ„"""
    if isinstance(obs, dict):
        return {k: get_obs_shape(v) for k, v in obs.items()}
    elif isinstance(obs, np.ndarray):
        return obs.shape
    else:
        return type(obs)


if __name__ == "__main__":
    main()