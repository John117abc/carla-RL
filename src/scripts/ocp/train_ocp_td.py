"""
æ•°æ®é‡‡é›†ä¸ç¯å¢ƒæµ‹è¯•è„šæœ¬ã€‚
ç”¨äºéªŒè¯ CarlaEnv æ˜¯å¦èƒ½æ­£å¸¸ reset/stepï¼Œå¹¶æ‰“å°è§‚æµ‹ã€åŠ¨ä½œã€å¥–åŠ±ç­‰ä¿¡æ¯ã€‚
å¯é€‰ä¿å­˜å›¾åƒæˆ–çŠ¶æ€æ—¥å¿—ã€‚
"""

import os
import numpy as np
import cv2
from src.utils import (load_config,get_logger,
                       setup_code_environment,
                       average_ocp_list)
from src.agents import OcpAgent
from src.buffer import Trajectory
# æ·»åŠ é¡¹ç›®æºç è·¯å¾„
# sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import gymnasium as gym
from src.envs.carla_env import CarlaEnv

logger = get_logger('train_ocp')


def save_image(obs, step: int, save_dir: str = "debug_images"):
    """ä¿å­˜è§‚æµ‹ä¸­çš„å›¾åƒï¼ˆå‡è®¾ obs æ˜¯ dict ä¸”åŒ…å« 'image'ï¼‰"""
    os.makedirs(save_dir, exist_ok=True)
    if isinstance(obs, dict) and 'image' in obs:
        img = obs['image']
        # å¦‚æœæ˜¯ (C, H, W)ï¼Œè½¬ä¸º (H, W, C)
        if img.shape[0] == 3:
            img = np.transpose(img, (1, 2, 0))
        img = (img * 255).astype(np.uint8)
        cv2.imwrite(f"{save_dir}/step_{step:04d}.png", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
    elif isinstance(obs, np.ndarray) and obs.ndim == 3:
        img = (obs * 255).astype(np.uint8)
        if img.shape[0] == 3:
            img = np.transpose(img, (1, 2, 0))
        cv2.imwrite(f"{save_dir}/step_{step:04d}.png", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))


def main():
    logger.info('å¼€å§‹è¯»å–é…ç½®æ–‡ä»¶...')
    carla_config = load_config('configs/carla.yaml')['word_01']
    env_config = load_config('configs/env.yaml')
    sys_config = load_config('configs/sys.yaml')
    rl_config = load_config('configs/rl.yaml')
    train_config = rl_config['rl']
    device = setup_code_environment(sys_config)
    action_repeat = env_config['world']['action_repeat']
    # å¯ç”¨sumoæ§åˆ¶äº¤é€š
    sumo_config = None
    if env_config['traffic']['enable_sumo']:
        sumo_config = load_config('configs/sumo.yaml')

    history = []
    logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– CARLA ç¯å¢ƒ...")
    env = CarlaEnv(
        render_mode=None,
        carla_config=carla_config,
        sumo_config=sumo_config,
        env_config=env_config
    )
    try:
        agent = OcpAgent(env=env, rl_config=rl_config, device=device)
        if train_config['continue_ocp']:
            logger.info("å¼€å§‹è¯»å–æ™ºèƒ½ä½“å‚æ•°...")
            checkpoint = agent.load(train_config["model_path_ocp"])

        logger.info("ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")
        logger.info(f"è§‚æµ‹ç©ºé—´: {env.observation_space}")
        logger.info(f"åŠ¨ä½œç©ºé—´: {env.action_space}")

        num_episodes = train_config["num_episodes"]
        global_step = 0
        episode = 0
        while episode < num_episodes:
            state, info = env.reset()
            state = state['ocp_obs']
            total_reward = 0.0
            done = False
            states, actions, rewards, infos, log_probs = [], [], [], [], []
            initial_state = state.copy()
            episode_step = 0
            global_step += 1  # global_stepä»£è¡¨â€œå†³ç­–æ­¥â€æ•°
            loss = None
            while not done:
                # å†³ç­–æ—¶åˆ»
                if episode_step % action_repeat == 0:
                    action, log_prob = agent.select_action(state)

                    # åˆå§‹åŒ–ç´¯è®¡ reward å’Œä¸´æ—¶ info åˆ—è¡¨
                    accumulated_reward = 0.0
                    temp_infos = []
                    accumulated_state = []
                    # æ‰§è¡Œ action_repeat æ¬¡åŠ¨ä½œ
                    for _ in range(action_repeat):
                        accumulated_state.append(state[1])
                        if done:
                            break
                        next_obs, reward, _, _, info = env.step(action)
                        next_state = next_obs['ocp_obs']
                        done = info['collision'] or info['off_route'] or info['TimeLimit.truncated']

                        accumulated_reward += reward
                        temp_infos.append(info)
                        total_reward += reward
                        episode_step += 1  # ç¯å¢ƒ step

                        # æ›´æ–°å‚æ•°
                        if agent.buffer.should_start_training():
                            loss = agent.update()

                        # è®°å½•åŸå§‹ step æ—¥å¿—
                        if (global_step * action_repeat + episode_step) % train_config["log_interval"] == 0:
                            logger.info(f"  EnvStep {episode_step}: reward={reward:.3f}")

                    #æ”¶é›†ä¸€ä¸ªâ€œå†³ç­–å‘¨æœŸâ€çš„ transition
                    states.append(average_ocp_list(accumulated_state))
                    actions.append(action)
                    rewards.append(accumulated_reward)
                    log_probs.append(log_prob)
                    # åˆå¹¶ infoï¼šå¯ä»¥ç”¨æœ€åä¸€æ­¥ï¼Œæˆ–è‡ªå®šä¹‰ï¼ˆå¦‚æ˜¯å¦æœ‰ collisionï¼‰
                    final_info = temp_infos[-1] if temp_infos else info
                    infos.append(final_info)

                    # æ›´æ–°çŠ¶æ€
                    state = next_state

                    # æ›´æ–°æƒ©ç½šå‚æ•°
                    agent.update_penalty(env.step_count)  # env.step_count æ˜¯ç¯å¢ƒæ€»æ­¥æ•°

                    global_step += 1  # å†³ç­–æ­¥ +1

                    # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                    if done:
                        logger.info(f"  Episode ç»“æŸ (info={final_info})")
                        break

                else:
                    raise RuntimeError("é€»è¾‘é”™è¯¯ï¼šéå†³ç­–æ­¥ä¸åº”åœ¨æ­¤å¾ªç¯ä¸­")

            # æ„å»º trajectoryï¼ˆç°åœ¨ states/actions/rewards éƒ½æ˜¯å†³ç­–æ­¥çº§åˆ«çš„ï¼‰
            total_cost, total_constraint = agent.compute_total_cost_and_constraint(states, actions)
            trajectory = Trajectory(
                initial_state=initial_state,
                states=states,
                actions=actions,
                rewards=rewards,
                infos=infos,
                total_cost=total_cost,
                total_constraint=total_constraint,
                path_id=env.current_path_id,
                horizon=len(states),
                log_probs=log_probs
            )
            agent.buffer.handle_new_trajectory(trajectory)

            if loss is not None:
                logger.info(f"è®­ç»ƒæŸå¤±: actor_loss:{loss['actor_loss']:.5f},critic_loss:{loss['critic_loss']:.5f},"
                            f"æƒ©ç½šå‚æ•°ï¼š{agent.init_penalty:.5f}")
                loss.update({
                    'global_step': global_step
                })
                history.append(loss)

            episode += 1

            # ä¿å­˜æ¨¡å‹
            if episode % train_config["save_freq"] == 0:
                logger.info(f"å¼€å§‹ä¿å­˜æ¨¡å‹ï¼š  Step {global_step}: total={total_reward:.2f}")
                save_info = {
                    'rl_config':rl_config,
                    'global_step':global_step,
                    'map':env_config['world']['map'],
                    'history_loss':history.copy(),
                }
                agent.save(save_info)

    except Exception as e:
        logger.error(f"ç¯å¢ƒè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        logger.info("\næ­£åœ¨å…³é—­ç¯å¢ƒ...")
        env.close()
        logger.info("æµ‹è¯•ç»“æŸã€‚")


def get_obs_shape(obs):
    """è¾…åŠ©å‡½æ•°ï¼šé€’å½’æ‰“å°è§‚æµ‹ç»“æ„"""
    if isinstance(obs, dict):
        return {k: get_obs_shape(v) for k, v in obs.items()}
    elif isinstance(obs, np.ndarray):
        return obs.shape
    else:
        return type(obs)


if __name__ == "__main__":
    main()