# carla-RL  
**åŸºäºCARLAä¸SUMOçš„è”åˆä»¿çœŸå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶**  
å…¼å®¹ Gymnasium ç¯å¢ƒä¸ Stable-Baselines3ï¼Œæ”¯æŒå¤šåœºæ™¯é…ç½®åŒ–è®­ç»ƒ

---

## ğŸ“Œ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªé«˜æ•ˆã€çµæ´»çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¹³å°ï¼Œé€šè¿‡æ•´åˆ **CARLA é«˜ä¿çœŸé©¾é©¶ä»¿çœŸ** ä¸ **SUMO äº¤é€šæµä»¿çœŸ**ï¼Œå®ç°äº¤é€šåœºæ™¯çš„ç²¾ç»†åŒ–å»ºæ¨¡ä¸æ™ºèƒ½ä½“å†³ç­–è®­ç»ƒã€‚æ ¸å¿ƒç‰¹ç‚¹åŒ…æ‹¬ï¼š

- **å…¼å®¹ä¸»æµç”Ÿæ€**ï¼šåŸºäº Gymnasium æ ‡å‡†ç¯å¢ƒæ¥å£ï¼Œæ— ç¼å¯¹æ¥ Stable-Baselines3 å¼ºåŒ–å­¦ä¹ ç®—æ³•åº“  
- **å¤šåœºæ™¯æ”¯æŒ**ï¼šæ”¯æŒ CARLA å¤šç§åœ°å›¾ä¸å¤©æ°”æ¡ä»¶ï¼Œå¯é…ç½® SUMO äº¤é€šæµï¼ˆè½¦è¾†ã€è·¯å£ã€ä¿¡å·ç¯ï¼‰  
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šé€šè¿‡ `src` ç›®å½•ä¸‹çš„æ¨¡å—åŒ–ç»„ä»¶ï¼ˆç¯å¢ƒã€æ¨¡å‹ã€ç¼“å­˜ç­‰ï¼‰å®ç°åŠŸèƒ½æ‰©å±•  
- **å¼ºåŒ–å­¦ä¹ è®­ç»ƒ**ï¼šæä¾›ä»ç¯å¢ƒåˆå§‹åŒ–ã€ä»¿çœŸåŒæ­¥åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´æµç¨‹  

---

## ğŸ›  åŠŸèƒ½ç‰¹ç‚¹

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| âœ… è”åˆä»¿çœŸ | CARLA è´Ÿè´£è½¦è¾†åŠ¨åŠ›å­¦ä¸ä¼ æ„Ÿå™¨ä»¿çœŸï¼ŒSUMO è´Ÿè´£å®è§‚äº¤é€šæµç®¡ç†ï¼Œé€šè¿‡è‡ªå®šä¹‰åŒæ­¥æ¨¡å—ä¿éšœæ—¶é—´æ­¥ä¸€è‡´ |
| âœ… é…ç½®é©±åŠ¨ | é€šè¿‡ `configs` ç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶çµæ´»å®šä¹‰ç¯å¢ƒå‚æ•°ï¼ˆåœ°å›¾ã€å¤©æ°”ï¼‰ã€SUMO åœºæ™¯ï¼ˆ`.net` æ–‡ä»¶ã€`.rou` æ–‡ä»¶ï¼‰ã€è®­ç»ƒè¶…å‚ |
| âœ… ç®—æ³•æ”¯æŒ | å¼€ç®±å³ç”¨çš„ Stable-Baselines3 ç®—æ³•ï¼ˆå¦‚ PPOã€A2Cã€SACï¼‰ï¼Œæ”¯æŒè‡ªå®šä¹‰ç­–ç•¥ç½‘ç»œ |
| âœ… ç¼“å­˜æœºåˆ¶ | `buffer` æ¨¡å—æä¾›ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œé€‚ç”¨äºéœ€è¦å›æ”¾çš„ç®—æ³•ï¼ˆå¦‚ DQNï¼‰ |
| âœ… å®ç”¨å·¥å…· | `utils` ç›®å½•æä¾›çŠ¶æ€é¢„å¤„ç†ã€åŠ¨ä½œæ˜ å°„ã€æ—¥å¿—è®°å½•ç­‰è¾…åŠ©åŠŸèƒ½ |

---

## ğŸ§° ç¯å¢ƒæ­å»º

### 1. ç³»ç»Ÿè¦æ±‚
- Linux/macOSï¼ˆæ¨è Ubuntu 20.04+ï¼‰
- Python 3.8+
- [CARLA Simulator 0.9.16+](https://carla.org/)
- [SUMO 1.16+](https://sumo.dlr.de/docs/Installing/index.html)

### 2. å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/John117abc/carla-RL.git
cd carla-RL

# åˆ›å»ºå¹¶æ¿€æ´»Pythonè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv rl-env
source rl-env/bin/activate  # Windows: rl-env\Scripts\activate

# å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt
# æˆ–æ‰‹åŠ¨å®‰è£…å…³é”®åŒ…
pip install gymnasium stable-baselines3 numpy pysumo
```
### 3.âš¡å¿«é€Ÿå…¥é—¨
```bash
1. å¯åŠ¨ä»¿çœŸå™¨
bash

# ç»ˆç«¯1ï¼šå¯åŠ¨CARLAæœåŠ¡å™¨
./CarlaUE4.sh  # Linux/macOS
# æˆ–
CarlaUE4.exe    # Windows

# ç»ˆç«¯2ï¼šå¯åŠ¨SUMOä»¿çœŸ
sumo -c src/envs/carla_sumo_env/town06_opt_env/Town06_Opt.sumocfg

# è®­ç»ƒè„šæœ¬ç›®å½•
src/scripts

carla-RL/
â”œâ”€â”€ carla_agents/        # CARLAæ™ºèƒ½ä½“ä»£ç ï¼Œä»carlaAPIä¸­è¿ç§»è¿‡æ¥çš„ï¼Œå®ç°è·¯å¾„è§„åˆ’
â”œâ”€â”€ carla_note/          # carlaçš„ä¸€äº›åŸºç¡€ä½¿ç”¨æ•™ç¨‹
â”œâ”€â”€ src/                 # æ ¸å¿ƒæºç ç›®å½•
â”‚   â”œâ”€â”€ agents/          # æ™ºèƒ½ä½“å®ç°
â”‚   â”œâ”€â”€ buffer/          # ç»éªŒå›æ”¾ç¼“å†²åŒº
â”‚   â”œâ”€â”€ carla_utils/     # CARLAä»¿çœŸå·¥å…·
â”‚   â”œâ”€â”€ configs/         # é…ç½®æ–‡ä»¶ï¼ˆç¯å¢ƒ/è®­ç»ƒï¼‰
â”‚   â”œâ”€â”€ envs/            # Gymnasiumç¯å¢ƒå°è£…
â”‚   â”œâ”€â”€ models/          # ç¥ç»ç½‘ç»œæ¨¡å‹
â”‚   â”œâ”€â”€ scripts/         # è®­ç»ƒ/è¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ sumo_sync/       # SUMOåŒæ­¥é€»è¾‘
â”‚   â””â”€â”€ utils/           # å·¥å…·å‡½æ•°ï¼ˆæ—¥å¿—/å¯è§†åŒ–ï¼‰
â”œâ”€â”€ sumo_sync/           # SUMOç½‘ç»œæ–‡ä»¶
â”œâ”€â”€ requirements.txt     # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ LICENSE              # å¼€æºåè®®
â””â”€â”€ README.md            # é¡¹ç›®è¯´æ˜
```
### 4.ğŸš€ä½¿ç”¨ç¤ºä¾‹

```bash
"""
æ•°æ®é‡‡é›†ä¸ç¯å¢ƒæµ‹è¯•è„šæœ¬ã€‚
ç”¨äºéªŒè¯ CarlaEnv æ˜¯å¦èƒ½æ­£å¸¸ reset/stepï¼Œå¹¶æ‰“å°è§‚æµ‹ã€åŠ¨ä½œã€å¥–åŠ±ç­‰ä¿¡æ¯ã€‚
å¯é€‰ä¿å­˜å›¾åƒæˆ–çŠ¶æ€æ—¥å¿—ã€‚
"""

import os
import numpy as np
import cv2
from src.utils import (load_config,get_logger,
                       setup_code_environment)
from src.agents import OcpAgent
from src.buffer import Trajectory

import gymnasium as gym
from src.envs.carla_env import CarlaEnv

logger = get_logger('train_ocp')

def main():
    logger.info('å¼€å§‹è¯»å–é…ç½®æ–‡ä»¶...')
    carla_config = load_config('configs/carla.yaml')['word_01']
    env_config = load_config('configs/env.yaml')
    sys_config = load_config('configs/sys.yaml')
    rl_config = load_config('configs/rl.yaml')
    train_config = rl_config['rl']
    device = setup_code_environment(sys_config)
    # å¯ç”¨sumoæ§åˆ¶äº¤é€š
    sumo_config = None
    if env_config['traffic']['enable_sumo']:
        sumo_config = load_config('configs/sumo.yaml')

    history = []
    logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ– CARLA ç¯å¢ƒ...")
    env = CarlaEnv(
        render_mode=None,
        carla_config=carla_config,
        sumo_config=sumo_config,
        env_config=env_config
    )
    try:
        agent = OcpAgent(env=env, rl_config=rl_config, device=device)
        if train_config['continue_ocp']:
            logger.info("å¼€å§‹è¯»å–æ™ºèƒ½ä½“å‚æ•°...")
            checkpoint = agent.load(train_config["model_path_ocp"])
            # if not env.is_eval:
            #     # è¯»å–å½’ä¸€åŒ–å‚æ•°
            #     env.ocp_normalizer.load_state_dict(checkpoint['ocp_normalizer'])

        logger.info("ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")
        logger.info(f"è§‚æµ‹ç©ºé—´: {env.observation_space}")
        logger.info(f"åŠ¨ä½œç©ºé—´: {env.action_space}")

        num_episodes = train_config["num_episodes"]
        global_step = 0
        episode = 0
        while episode < num_episodes:
            logger.info(f"\nå¼€å§‹ç¬¬ {episode + 1} è½®æµ‹è¯•...")
            state, info = env.reset()
            state = state['ocp_obs']
            logger.info(f"åˆå§‹è§‚æµ‹ç±»å‹: {type(state)}, å½¢çŠ¶/ç»“æ„: {get_obs_shape(state)}")
            total_reward = 0.0
            done = False
            states, actions, rewards, infos ,log_probs= [], [], [], [],[]
            initial_state = state.copy()
            while not done:
                action,log_prob = agent.select_action(state)
                next_obs, reward, _, _, info = env.step(action)
                next_state = next_obs['ocp_obs']
                done = info['collision'] or info['off_route'] or info['TimeLimit.truncated']
                total_reward += reward
                # æ•°æ®åŠ å…¥buffer
                actions.append(action)
                states.append(state[1])
                rewards.append(reward)
                log_probs.append(log_prob)
                infos.append(info)
                state = next_state

                # æ›´æ–°æƒ©ç½šå‚æ•°
                agent.update_penalty(env.step_count)
                # æ‰“å°å…³é”®ä¿¡æ¯
                if global_step % train_config["log_interval"] == 0:
                    logger.info(f"  Step {global_step}: reward={reward:.3f}, total={total_reward:.2f}")
                    if 'speed' in info:
                        logger.info(f"    é€Ÿåº¦: {info['speed']:.2f} km/h")

                if done:
                    logger.info(f"  Episode ç»“æŸ (info={info})")
                    break
                global_step += 1
            # è®¡ç®— total_cost å’Œ total_constraint
            total_cost, total_constraint = agent.compute_total_cost_and_constraint(states, actions)
            trajectory = Trajectory(initial_state=initial_state,
                                    states=states,actions=actions,
                                    rewards=rewards,
                                    infos=infos,
                                    total_cost=total_cost,
                                    total_constraint=total_constraint,
                                    path_id=env.current_path_id,
                                    horizon=len(states),
                                    log_probs = log_probs)
            # åŠ å…¥buffer
            agent.buffer.handle_new_trajectory(trajectory)

            # æ›´æ–°å‚æ•°
            loss = None
            if agent.buffer.should_start_training():
                loss = agent.update()
            logger.info(f"ç¬¬ {episode} è½®å®Œæˆï¼Œæ€»å¥–åŠ±: {total_reward:.2f}")

            if loss is not None:
                logger.info(f"è®­ç»ƒæŸå¤±: actor_loss:{loss['actor_loss']:.5f},critic_loss:{loss['critic_loss']:.5f},"
                            f"æƒ©ç½šå‚æ•°ï¼š{agent.init_penalty:.5f}")
                loss.update({
                    'global_step': global_step
                })
                history.append(loss)

            episode += 1

            # ä¿å­˜æ¨¡å‹
            if episode % train_config["save_freq"] == 0:
                logger.info(f"å¼€å§‹ä¿å­˜æ¨¡å‹ï¼š  Step {global_step}: total={total_reward:.2f}")
                save_info = {
                    'rl_config':rl_config,
                    'global_step':global_step,
                    'map':env_config['world']['map'],
                    'history_loss':history.copy(),
                    # 'ocp_normalizer': env.ocp_normalizer.state_dict()
                }
                agent.save(save_info)

    except Exception as e:
        logger.error(f"ç¯å¢ƒè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        logger.info("\næ­£åœ¨å…³é—­ç¯å¢ƒ...")
        env.close()
        logger.info("æµ‹è¯•ç»“æŸã€‚")


def get_obs_shape(obs):
    """è¾…åŠ©å‡½æ•°ï¼šé€’å½’æ‰“å°è§‚æµ‹ç»“æ„"""
    if isinstance(obs, dict):
        return {k: get_obs_shape(v) for k, v in obs.items()}
    elif isinstance(obs, np.ndarray):
        return obs.shape
    else:
        return type(obs)


if __name__ == "__main__":
    main()
```

### 5.ğŸŒŸ è´¡çŒ®æŒ‡å—

    æäº¤ Issueï¼šæŠ¥å‘Š Bug æˆ–æå‡ºåŠŸèƒ½å»ºè®®
    Fork é¡¹ç›®ï¼šåœ¨ GitHub ä¸Š Fork æœ¬é¡¹ç›®
    æäº¤ PRï¼šå®Œå–„æ–‡æ¡£ / æ·»åŠ æ–°åŠŸèƒ½
    äº¤æµï¼šé€šè¿‡ GitHub Issues è®¨è®º

    æ¬¢è¿ Star æ”¯æŒï¼â­ï¸

ğŸ“œ å¼€æºåè®®
æœ¬é¡¹ç›®é‡‡ç”¨ MIT License å¼€æºï¼Œå…è®¸å•†ä¸šå’Œéå•†ä¸šä½¿ç”¨ï¼Œæ— éœ€é¢å¤–æˆæƒã€‚
ä½¿ç”¨æ—¶è¯·ä¿ç•™åŸä½œè€…ä¿¡æ¯ï¼ˆJohn117abcï¼‰ã€‚